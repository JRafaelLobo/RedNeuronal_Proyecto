{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f7a942d-9cc2-4955-be06-b7949fdd5196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import DnnLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "339c97ad-9dff-4b9c-a8a6-2e8347cfcb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salida con activación: [[0.05 0.14]]\n",
      "Salida lineal: [[0.05 0.14]]\n",
      "Sigmoid: [0.5        0.8807971  0.26894143]\n"
     ]
    }
   ],
   "source": [
    "# Crear datos de entrada\n",
    "x = np.array([[0.5, -0.2, 0.1]])\n",
    "\n",
    "# Crear capa densa: 3 entradas, 2 salidas, con activación ReLU\n",
    "layer = DnnLib.DenseLayer(3, 2, DnnLib.ActivationType.RELU)\n",
    "\n",
    "# Modificar manualmente los pesos y bias\n",
    "layer.weights = np.array([[0.1, 0.2, 0.3],\n",
    "                          [0.4, 0.5, 0.6]])\n",
    "layer.bias = np.array([0.01, -0.02])\n",
    "\n",
    "# Forward con activación\n",
    "y = layer.forward(x)\n",
    "print(\"Salida con activación:\", y)\n",
    "\n",
    "# Forward lineal (sin activación)\n",
    "y_lin = layer.forward_linear(x)\n",
    "print(\"Salida lineal:\", y_lin)\n",
    "\n",
    "# Aplicar activaciones directamente\n",
    "print(\"Sigmoid:\", DnnLib.sigmoid(np.array([0.0, 2.0, -1.0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c082de18-6282-48d7-9e1d-b826ddbbce07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
